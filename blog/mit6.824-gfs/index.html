<!DOCTYPE html>
<html lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MIT6.824 GFS</title>
    <meta charset="utf-8">
    <meta name="description" content="Ladder@This article introduces the Google File System (GFS) paper published in 2003, which proposed a distributed file system designed to store large volumes of data reliably, meeting Google&rsquo;s data storage needs. This write-up reflects on the design goals, trade-offs, and architectural choices of GFS.
Introduction GFS is a distributed file system developed by Google to meet the needs of data-intensive applications, using commodity hardware to provide a scalable and fault-tolerant solution.">
    <meta name="author" content="NoneBack">
    <link rel="canonical" href="https://noneback.github.io/blog/mit6.824-gfs/">
        <meta name="google-site-verification" content="xxx">

    <link rel="alternate" type="application/rss+xml" href="https://noneback.github.io//index.xml" title="NoneBack">

    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-H0SRTJWPEK"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-H0SRTJWPEK', { 'anonymize_ip': false });
}
</script>



<script async defer data-website-id="43dc9e5a-7ab8-482e-94df-100975b5d2c8" src="https://umami-blog-pi.vercel.app/noneback-blog"></script>

    <meta property="og:title" content="MIT6.824 GFS" />
<meta property="og:description" content="This article introduces the Google File System (GFS) paper published in 2003, which proposed a distributed file system designed to store large volumes of data reliably, meeting Google&rsquo;s data storage needs. This write-up reflects on the design goals, trade-offs, and architectural choices of GFS.
Introduction GFS is a distributed file system developed by Google to meet the needs of data-intensive applications, using commodity hardware to provide a scalable and fault-tolerant solution." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://noneback.github.io/blog/mit6.824-gfs/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2021-09-09T00:44:24+08:00" />
<meta property="article:modified_time" content="2021-09-09T00:44:24+08:00" />


<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="MIT6.824 GFS"/>
<meta name="twitter:description" content="This article introduces the Google File System (GFS) paper published in 2003, which proposed a distributed file system designed to store large volumes of data reliably, meeting Google&rsquo;s data storage needs. This write-up reflects on the design goals, trade-offs, and architectural choices of GFS.
Introduction GFS is a distributed file system developed by Google to meet the needs of data-intensive applications, using commodity hardware to provide a scalable and fault-tolerant solution."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://noneback.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "MIT6.824 GFS",
      "item": "https://noneback.github.io/blog/mit6.824-gfs/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MIT6.824 GFS",
  "name": "MIT6.824 GFS",
  "description": "This article introduces the Google File System (GFS) paper published in 2003, which proposed a distributed file system designed to store large volumes of data reliably, meeting Google\u0026rsquo;s data storage needs. This write-up reflects on the design goals, trade-offs, and architectural choices of GFS.\nIntroduction GFS is a distributed file system developed by Google to meet the needs of data-intensive applications, using commodity hardware to provide a scalable and fault-tolerant solution.",
  "keywords": [
    "Distributed System", "GFS", "DFS"
  ],
  "articleBody": "This article introduces the Google File System (GFS) paper published in 2003, which proposed a distributed file system designed to store large volumes of data reliably, meeting Google’s data storage needs. This write-up reflects on the design goals, trade-offs, and architectural choices of GFS.\nIntroduction GFS is a distributed file system developed by Google to meet the needs of data-intensive applications, using commodity hardware to provide a scalable and fault-tolerant solution.\nBackground Component Failures as the Norm: In GFS, component failures are treated as normal events rather than exceptions. GFS uses inexpensive hardware to build a reliable service. Each machine has a certain probability of failure, resulting in a binomial distribution of overall system failures. The key challenge is to ensure the system remains available through redundancy and rapid failover.\nMassive Files: Files in GFS can be extremely large, ranging from several hundred megabytes to tens of gigabytes. GFS favors large files rather than many small files. Managing a large number of small files in a distributed system can lead to increased metadata overhead, inefficient caching, and greater inode usage.\nSequential Access: Most file modifications append data to the end of files rather than random modifications, and reads are generally sequential. GFS is optimized for sequential writes, especially for appending data. Random writes are not well-supported and do not guarantee consistency.\nCollaborative Design: The API and file system are designed collaboratively to improve efficiency and flexibility. GFS provides an API similar to POSIX but includes additional optimizations to better match Google’s workload.\nDesign Goals Storage Capacity GFS is designed to manage millions of files, most of which are at least 100 MB in size. Files of several gigabytes are common, but GFS also supports smaller files without specific optimization.\nWorkload Read Workload Large-Scale Sequential Reads: Large-scale sequential data retrieval using disk I/O. Small-Scale Random Reads: Small-scale random data retrieval, optimized through techniques such as request batching. Write Workload Primarily large-scale sequential writes, typically appending data to the end of files. GFS supports concurrent data appends from multiple clients, with atomic guarantees and synchronization.\nBandwidth vs. Latency High sustained bandwidth is prioritized over low latency, given the typical workloads of GFS.\nFault Tolerance GFS continuously monitors its state to detect and recover from component failures, which are treated as common occurrences.\nOperations and Interfaces GFS provides traditional file system operations such as file creation, deletion, and reading, along with features like snapshots and atomic record append.\nSnapshots create file or directory copies, while atomic record append guarantees that data is appended atomically.\nArchitecture The architecture of GFS follows a Master-Slave design, consisting of a single Master node and multiple Chunk Servers.\nThe Master and Chunk Servers are logical concepts and do not necessarily refer to specific physical machines.\nGFS provides a client library (SDK) that allows clients to access the system, abstracting the underlying complexity. File data is divided into chunks and stored across multiple Chunk Servers, with replication for reliability. The Master manages metadata such as namespace, chunk locations, and more.\nComponent Overview Client Clients in GFS are application processes that use the GFS SDK for seamless integration. Key functionalities of the client include:\nCaching: Cache metadata obtained from the Master to reduce communication overhead. Encapsulation: Encapsulate retries, request splitting, and checksum validation. Optimization: Perform request batching, load balancing, and caching to enhance efficiency. Mapping: Map file operations to chunk-based ones, such as converting (filename, offset) into (chunk index, offset). Master The Master maintains all metadata, including the namespace, file-to-chunk mappings, and chunk versioning. Key functionalities include:\nMonitoring: Track Chunk Server status and data locations using heartbeats. Directory Tree Management: Manage the hierarchical file system structure with efficient locking mechanisms. Mapping Management: Maintain mappings between files and chunks for fast lookups. Fault Tolerance: Utilize checkpointing and Raft-style multi-replica backups to recover from Master failures. System Scheduling: Manage chunk replication, garbage collection, lease distribution, and primary Chunk Server selection. Metadata is stored in memory for performance reasons, resulting in a simplified design, but making checkpointing and logging crucial to ensure recovery.\nChunk Server Chunk Servers are responsible for storing data, with each file chunk being saved as a Linux file. Chunk Servers also perform data integrity checks and report health information to the Master regularly.\nKey Concepts and Mechanisms Chunk Size Chunks are the logical units for storing data in GFS, with each chunk typically sized at 64 MB. The chunk size balances metadata overhead, caching efficiency, data locality, and fault tolerance.\nSmall chunks increase metadata load on the Master, whereas larger chunks can create data hot spots and fragmentation.\nLease Mechanism GFS uses a lease mechanism to ensure consistency between chunk replicas. When concurrent write requests occur, the Master selects a Chunk Server to be the primary. The primary node assigns an order to client operations, ensuring concurrent operations are executed consistently.\nThis mechanism reduces the coordination load on the Master and allows data to be appended atomically.\nChunk Versioning The versioning system is used to ensure that only the latest chunk version is valid. The Master increments the version whenever a lease is granted, and a new version number is committed after acknowledgment from the primary.\nVersioning helps determine the freshness of data during recoveries.\nControl Flow vs. Data Flow GFS separates control flow and data flow to optimize data transfers. Control commands are issued separately from data transfers, enabling efficient utilization of network topology.\nData is sent using a pipeline approach between Chunk Servers, which minimizes network overhead and uses cache effectively.\nData Integrity Chunks are split into 64 KB blocks, each with a corresponding checksum for data integrity. These checksums are used to verify data during read operations.\nChecksums are stored separately from the data, providing an additional layer of reliability.\nFault Tolerance and Replication Chunks are stored in multiple replicas across different Chunk Servers for reliability. The Master detects Chunk Server failures via heartbeats and manages replication to meet desired redundancy levels.\nData integrity failures or Chunk Server disconnections trigger replication to maintain availability.\nConsistency GFS has a relaxed consistency model. It provides eventual consistency and does not guarantee strong consistency.\nIn practice, operations such as atomic record append ensure data integrity during appends but may not eliminate duplicate writes. Random writes are not consistently managed.\nSummary GFS demonstrates how practical design trade-offs, driven by specific business needs, can lead to an efficient and scalable distributed file system. It focuses on resilience, fault tolerance, and high throughput, making it ideal for Google’s data processing needs.\nIn distributed systems, scalability is often more important than single-node performance. GFS embraces this principle through large file management, redundancy, and workload distribution.\nReferences Google File System - GFS Paper Reading GFS Paper Summary GFS Paper Overview GFS Original Paper MIT6.824 Course ",
  "wordCount" : "1121",
  "inLanguage": "en",
  "datePublished": "2021-09-09T00:44:24+08:00",
  "dateModified": "2021-09-09T00:44:24+08:00",
  "author":{
    "@type": "Person",
    "name": "NoneBack"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://noneback.github.io/blog/mit6.824-gfs/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "NoneBack",
    "logo": {
      "@type": "ImageObject",
      "url": "https://noneback.github.io/favicon.ico"
    }
  }
}
</script>
    <link rel="icon" href="/images/avatar.jpeg" sizes="16x16">

<link rel="apple-touch-icon" href="/images/avatar.jpeg">

<link rel="manifest" href="/images/avatar.jpeg">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true },
                { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                { left: "\\begin{align}", right: "\\end{align}", display: true },
                { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
                { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                { left: "\\[", right: "\\]", display: true }
            ],
            
            throwOnError: false,
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
                "\\eqref": "\\href{###1}{(\\text{#1})}",
                "\\ref": "\\href{###1}{\\text{#1}}",
                "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.7.0/style.css" />

    
    
    <link rel="stylesheet" href="/css/main.min.ec28f09e946fc0df77c187fcd0d0ebde58fca6de8efb8e1620f3d45c32d4da88.css" integrity="sha256-7CjwnpRvwN93wYf80NDr3lj8pt6O&#43;44WIPPUXDLU2og=" crossorigin="anonymous" media="screen" />

    
    <link rel="stylesheet" href="/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css" />

    
    <script src="/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js"></script>
    <script>hljs.highlightAll();</script>

    <script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script>
    </head>
<body>
      <main class="wrapper"><nav class="navigation">
    <section class="container">
        <a class="navigation-brand" href="/">
            HOME
        </a>
        <input type="checkbox" id="menu-toggle" />
        <label class="menu-button float-right" for="menu-toggle">
            <span></span><span></span><span></span>
        </label>
        
        <ul class="navigation-list" id="navigation-list">
            
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/blog">Blog</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/tags">Tags</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/archives">Archive</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="https://umami-blog-pi.vercel.app/share/q7qW5hQ16F8cTkBD/noneback.github.io">Dashboard</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/about/">About</a>
            </li>
            
            

            <li class="navigation-item menu-separator">
                <span>|</span>
            </li>

            
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://github.com/noneback"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a>
            </li>
            
            

            <li class="navigation-item navigation-dark">
                <button id="mode" type="button" aria-label="toggle user light or dark theme">
                    <span class="toggle-dark"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></span>
                    <span class="toggle-light"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></span>
                </button>
            </li>

            
            
            
            
            
            
            
            <li class="navigation-item navigation-language">
                <a href="https://noneback.github.io/zh/">中</a>
            </li>
            
            
            
            
        </ul>
        
    </section>
</nav>
<div id="content">
<article class="blog-single">
  <header class="blog-title">
    <h1>MIT6.824 GFS</h1>
  </header>

  <p>
  <small>
    September 9, 2021&nbsp;· 1121 words&nbsp;· 6 min</small>

  <small>
      
      ·
      
      
      <a href="https://noneback.github.io/tags/gfs/">GFS</a>
      
      <a href="https://noneback.github.io/tags/mit6.824/">MIT6.824</a>
      
      <a href="https://noneback.github.io/tags/paper-reading/">Paper Reading</a>
      
    </small>
  
<p>

  <div class="blog-toc">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#background">Background</a></li>
      </ul>
    </li>
    <li><a href="#design-goals">Design Goals</a>
      <ul>
        <li><a href="#storage-capacity">Storage Capacity</a></li>
        <li><a href="#workload">Workload</a></li>
        <li><a href="#bandwidth-vs-latency">Bandwidth vs. Latency</a></li>
        <li><a href="#fault-tolerance">Fault Tolerance</a></li>
        <li><a href="#operations-and-interfaces">Operations and Interfaces</a></li>
      </ul>
    </li>
    <li><a href="#architecture">Architecture</a>
      <ul>
        <li><a href="#component-overview">Component Overview</a></li>
      </ul>
    </li>
    <li><a href="#key-concepts-and-mechanisms">Key Concepts and Mechanisms</a>
      <ul>
        <li><a href="#chunk-size">Chunk Size</a></li>
        <li><a href="#lease-mechanism">Lease Mechanism</a></li>
        <li><a href="#chunk-versioning">Chunk Versioning</a></li>
        <li><a href="#control-flow-vs-data-flow">Control Flow vs. Data Flow</a></li>
        <li><a href="#data-integrity">Data Integrity</a></li>
        <li><a href="#fault-tolerance-and-replication">Fault Tolerance and Replication</a></li>
        <li><a href="#consistency">Consistency</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>

  <section class="blog-content"><p>This article introduces the Google File System (GFS) paper published in 2003, which proposed a distributed file system designed to store large volumes of data reliably, meeting Google&rsquo;s data storage needs. This write-up reflects on the design goals, trade-offs, and architectural choices of GFS.</p>
<h2 id="introduction">Introduction</h2>
<p>GFS is a distributed file system developed by Google to meet the needs of data-intensive applications, using commodity hardware to provide a scalable and fault-tolerant solution.</p>
<h3 id="background">Background</h3>
<ol>
<li><strong>Component Failures as the Norm</strong>: In GFS, component failures are treated as normal events rather than exceptions.</li>
</ol>
<blockquote>
<p>GFS uses inexpensive hardware to build a reliable service. Each machine has a certain probability of failure, resulting in a binomial distribution of overall system failures. The key challenge is to ensure the system remains available through redundancy and rapid failover.</p>
</blockquote>
<ol start="2">
<li><strong>Massive Files</strong>: Files in GFS can be extremely large, ranging from several hundred megabytes to tens of gigabytes.</li>
</ol>
<blockquote>
<p>GFS favors large files rather than many small files. Managing a large number of small files in a distributed system can lead to increased metadata overhead, inefficient caching, and greater inode usage.</p>
</blockquote>
<ol start="3">
<li><strong>Sequential Access</strong>: Most file modifications append data to the end of files rather than random modifications, and reads are generally sequential.</li>
</ol>
<blockquote>
<p>GFS is optimized for sequential writes, especially for appending data. Random writes are not well-supported and do not guarantee consistency.</p>
</blockquote>
<ol start="4">
<li><strong>Collaborative Design</strong>: The API and file system are designed collaboratively to improve efficiency and flexibility.</li>
</ol>
<blockquote>
<p>GFS provides an API similar to POSIX but includes additional optimizations to better match Google&rsquo;s workload.</p>
</blockquote>
<h2 id="design-goals">Design Goals</h2>
<h3 id="storage-capacity">Storage Capacity</h3>
<p>GFS is designed to manage millions of files, most of which are at least 100 MB in size. Files of several gigabytes are common, but GFS also supports smaller files without specific optimization.</p>
<h3 id="workload">Workload</h3>
<h4 id="read-workload">Read Workload</h4>
<ol>
<li><strong>Large-Scale Sequential Reads</strong>: Large-scale sequential data retrieval using disk I/O.</li>
<li><strong>Small-Scale Random Reads</strong>: Small-scale random data retrieval, optimized through techniques such as request batching.</li>
</ol>
<h4 id="write-workload">Write Workload</h4>
<p>Primarily large-scale sequential writes, typically appending data to the end of files. GFS supports <strong>concurrent data appends</strong> from multiple clients, with atomic guarantees and synchronization.</p>
<h3 id="bandwidth-vs-latency">Bandwidth vs. Latency</h3>
<p>High <strong>sustained bandwidth</strong> is prioritized over low latency, given the typical workloads of GFS.</p>
<h3 id="fault-tolerance">Fault Tolerance</h3>
<p>GFS continuously monitors its state to detect and recover from component failures, which are treated as common occurrences.</p>
<h3 id="operations-and-interfaces">Operations and Interfaces</h3>
<p>GFS provides traditional file system operations such as file creation, deletion, and reading, along with features like <strong>snapshots</strong> and <strong>atomic record append</strong>.</p>
<blockquote>
<p>Snapshots create file or directory copies, while atomic record append guarantees that data is appended atomically.</p>
</blockquote>
<h2 id="architecture">Architecture</h2>
<p>The architecture of GFS follows a Master-Slave design, consisting of a single Master node and multiple Chunk Servers.</p>
<blockquote>
<p>The Master and Chunk Servers are logical concepts and do not necessarily refer to specific physical machines.</p>
</blockquote>
<p><img alt="GFS Architecture" src="https://tva1.sinaimg.cn/large/008i3skNly1gu6y6qm5t0j61i40nojuk02.jpg"></p>
<p>GFS provides a client library (SDK) that allows clients to access the system, abstracting the underlying complexity. File data is divided into chunks and stored across multiple Chunk Servers, with replication for reliability. The Master manages metadata such as namespace, chunk locations, and more.</p>
<h3 id="component-overview">Component Overview</h3>
<h4 id="client">Client</h4>
<p>Clients in GFS are application processes that use the GFS SDK for seamless integration. Key functionalities of the client include:</p>
<ul>
<li><strong>Caching</strong>: Cache metadata obtained from the Master to reduce communication overhead.</li>
<li><strong>Encapsulation</strong>: Encapsulate retries, request splitting, and checksum validation.</li>
<li><strong>Optimization</strong>: Perform request batching, load balancing, and caching to enhance efficiency.</li>
<li><strong>Mapping</strong>: Map file operations to chunk-based ones, such as converting <code>(filename, offset)</code> into <code>(chunk index, offset)</code>.</li>
</ul>
<h4 id="master">Master</h4>
<p>The Master maintains all metadata, including the namespace, file-to-chunk mappings, and chunk versioning. Key functionalities include:</p>
<ul>
<li><strong>Monitoring</strong>: Track Chunk Server status and data locations using heartbeats.</li>
<li><strong>Directory Tree Management</strong>: Manage the hierarchical file system structure with efficient locking mechanisms.</li>
<li><strong>Mapping Management</strong>: Maintain mappings between files and chunks for fast lookups.</li>
<li><strong>Fault Tolerance</strong>: Utilize checkpointing and Raft-style multi-replica backups to recover from Master failures.</li>
<li><strong>System Scheduling</strong>: Manage chunk replication, garbage collection, lease distribution, and primary Chunk Server selection.</li>
</ul>
<blockquote>
<p>Metadata is stored in memory for performance reasons, resulting in a simplified design, but making checkpointing and logging crucial to ensure recovery.</p>
</blockquote>
<h4 id="chunk-server">Chunk Server</h4>
<p>Chunk Servers are responsible for storing data, with each file chunk being saved as a Linux file. Chunk Servers also perform data integrity checks and report health information to the Master regularly.</p>
<h2 id="key-concepts-and-mechanisms">Key Concepts and Mechanisms</h2>
<h3 id="chunk-size">Chunk Size</h3>
<p>Chunks are the logical units for storing data in GFS, with each chunk typically sized at 64 MB. The chunk size balances metadata overhead, caching efficiency, data locality, and fault tolerance.</p>
<blockquote>
<p>Small chunks increase metadata load on the Master, whereas larger chunks can create data hot spots and fragmentation.</p>
</blockquote>
<h3 id="lease-mechanism">Lease Mechanism</h3>
<p>GFS uses a <strong>lease mechanism</strong> to ensure consistency between chunk replicas. When concurrent write requests occur, the Master selects a Chunk Server to be the <strong>primary</strong>. The primary node assigns an order to client operations, ensuring concurrent operations are executed consistently.</p>
<blockquote>
<p>This mechanism reduces the coordination load on the Master and allows data to be appended atomically.</p>
</blockquote>
<h3 id="chunk-versioning">Chunk Versioning</h3>
<p>The versioning system is used to ensure that only the latest chunk version is valid. The Master increments the version whenever a lease is granted, and a new version number is committed after acknowledgment from the primary.</p>
<blockquote>
<p>Versioning helps determine the freshness of data during recoveries.</p>
</blockquote>
<h3 id="control-flow-vs-data-flow">Control Flow vs. Data Flow</h3>
<p>GFS separates <strong>control flow</strong> and <strong>data flow</strong> to optimize data transfers. Control commands are issued separately from data transfers, enabling efficient utilization of network topology.</p>
<blockquote>
<p>Data is sent using a <strong>pipeline</strong> approach between Chunk Servers, which minimizes network overhead and uses cache effectively.</p>
</blockquote>
<h3 id="data-integrity">Data Integrity</h3>
<p>Chunks are split into 64 KB blocks, each with a corresponding checksum for data integrity. These checksums are used to verify data during read operations.</p>
<blockquote>
<p>Checksums are stored separately from the data, providing an additional layer of reliability.</p>
</blockquote>
<h3 id="fault-tolerance-and-replication">Fault Tolerance and Replication</h3>
<p>Chunks are stored in multiple replicas across different Chunk Servers for reliability. The Master detects Chunk Server failures via heartbeats and manages replication to meet desired redundancy levels.</p>
<blockquote>
<p>Data integrity failures or Chunk Server disconnections trigger replication to maintain availability.</p>
</blockquote>
<h3 id="consistency">Consistency</h3>
<p>GFS has a relaxed consistency model. It provides <strong>eventual consistency</strong> and does not guarantee strong consistency.</p>
<blockquote>
<p>In practice, operations such as <strong>atomic record append</strong> ensure data integrity during appends but may not eliminate duplicate writes. Random writes are not consistently managed.</p>
</blockquote>
<h2 id="summary">Summary</h2>
<p>GFS demonstrates how practical design trade-offs, driven by specific business needs, can lead to an efficient and scalable distributed file system. It focuses on resilience, fault tolerance, and high throughput, making it ideal for Google&rsquo;s data processing needs.</p>
<p>In distributed systems, scalability is often more important than single-node performance. GFS embraces this principle through large file management, redundancy, and workload distribution.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://spongecaptain.cool/post/paper/googlefilesystem/">Google File System - GFS Paper Reading</a></li>
<li><a href="https://tanxinyu.work/gfs-thesis/">GFS Paper Summary</a></li>
<li><a href="https://nxwz51a5wp.feishu.cn/docs/doccnNYeo3oXj6cWohseo6yB4id">GFS Paper Overview</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf">GFS Original Paper</a></li>
<li><a href="https://pdos.csail.mit.edu/6.824/schedule.html">MIT6.824 Course</a></li>
</ul>
</section>

  
  
  <div class="paginator">
    
    <a class="prev" href="https://noneback.github.io/blog/mit6.824-bigtable/">
      <svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375C21.4339 3.93962 21.3581 3.30535 21.1917 2.76787M3.77086 21.1546C1.9934 20.7777 0.973585 18.7264 1.08749 16.688C1.2668 13.479 1.15721 9.43135 1.00513 6.21507C0.87809 3.52811 3.12891 1.16316 5.51029 1.25008C9.76594 1.40542 15.377 1.20229 18.7912 1.00542C20.0864 0.930734 20.8406 1.63385 21.1917 2.76787M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787C23.1097 4.18217 23.13 12.4191 22.9004 16.3608C20.8478 24.0194 12.3061 23.6662 6.5 22.0658M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608C21.2797 23.0494 11.3665 22.9511 6.5 22.0658M9.94496 9C9.28897 9.61644 7.63215 10.997 6.04814 11.7966C5.98257 11.8297 5.98456 11.9753 6.05061 12.0063C7.05496 12.4779 8.92941 13.9264 9.94496 15M6.44444 11.9667C8.86549 12.0608 14 12 16 11" stroke="currentColor" stroke-linecap="round"/>
      </svg>
      <span>MIT6.824 Bigtable</span></a>
    
    
    <a class="next" href="https://noneback.github.io/blog/epoll-and-io%E5%A4%8D%E7%94%A8/"><span>Epoll and IO Multiplexing</span>
      <svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375C21.4339 3.93962 21.3581 3.30535 21.1917 2.76787M3.77086 21.1546C1.9934 20.7777 0.973585 18.7264 1.08749 16.688C1.2668 13.479 1.15721 9.43135 1.00513 6.21507C0.87809 3.52811 3.12891 1.16316 5.51029 1.25008C9.76594 1.40542 15.377 1.20229 18.7912 1.00542C20.0864 0.930734 20.8406 1.63385 21.1917 2.76787M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787C23.1097 4.18217 23.13 12.4191 22.9004 16.3608C20.8478 24.0194 12.3061 23.6662 6.5 22.0658M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608C21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966C16.0174 11.8297 16.0154 11.9753 15.9494 12.0063C14.945 12.4779 13.0706 13.9264 12.055 15M15.5556 11.9667C13.1345 12.0608 8 12 6 11" stroke="currentColor" stroke-linecap="round"/>
      </svg>
    </a>
    
  </div>
  

  


  
  
<div class="comments">
  <script>
      const getTheme = window.localStorage && window.localStorage.getItem("theme");
      let theme = getTheme === 'dark' ? 'dark' : 'light';
      let s = document.createElement('script');
      s.src = 'https://giscus.app/client.js';
      s.setAttribute('data-repo', 'noneback\/noneback.github.io');
      s.setAttribute('data-repo-id', 'MDEwOlJlcG9zaXRvcnkzMTAyNzgwNTc=');
      s.setAttribute('data-category', 'Announcements');
      s.setAttribute('data-category-id', 'DIC_kwDOEn53qc4Cj4-F');
      s.setAttribute('data-mapping', 'pathname');
      s.setAttribute('data-strict', '0');
      s.setAttribute('data-reactions-enabled', '1');
      s.setAttribute('data-emit-metadata', '0');
      s.setAttribute('data-input-position', 'top');
      s.setAttribute('data-theme', theme);
      s.setAttribute('data-lang', 'en');
      s.setAttribute('data-loading', 'lazy');
      s.setAttribute('crossorigin', 'anonymous');
      s.setAttribute('async', '');
      document.querySelector('div.comments').innerHTML = '';
      document.querySelector('div.comments').appendChild(s);
  </script>
</div>

</article>


        </div><footer class="footer">
  <p>&copy; 2025 <a href="https://noneback.github.io/">NoneBack</a>
    Powered by
    <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>
    <a href="https://github.com/guangzhengli/hugo-theme-ladder" rel="noopener" target="_blank">Ladder</a>
️  </p>
</footer>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211C22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257M21.7387 7.71865C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257C17.1684 -1.24629 7.83127 0.632493 4.27577 5.04257C2.88063 6.77451 -0.0433281 11.1668 1.38159 16.6571C2.27481 20.0988 5.17269 22.2936 8.19743 22.7725M20.7188 5.04257C22.0697 6.9404 24.0299 11.3848 22.3541 15.4153M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814C11.1703 6.98257 11.0247 6.98456 10.9937 7.05061C10.5221 8.05496 9.07362 9.92941 8 10.945M11.0333 7.44444C10.9392 9.86549 11 15 12 17" stroke="currentColor" stroke-linecap="round"/>
    </svg>
</a>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>

<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'Copy';

        function copyingDone() {
            copybutton.innerHTML = 'Copied';
            setTimeout(() => {
                copybutton.innerHTML = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });
        codeblock.parentNode.appendChild(copybutton);
    });
</script></main>
    </body><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <script>
      const images = Array.from(document.querySelectorAll(".blog-content img"));
      images.forEach(img => {
          mediumZoom(img, {
              margin: 10,  
              scrollOffset: 40,  
              container: null,  
              template: null,  
              background: 'rgba(0, 0, 0, 0.5)'
          });
      });
  </script>

  
  <script src="/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js" integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin="anonymous" defer></script></html>
