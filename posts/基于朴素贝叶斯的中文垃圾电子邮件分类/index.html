<!DOCTYPE html>
<html lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chinese Spam Email Classification Based on Naive Bayes</title>
    <meta charset="utf-8">
    <meta name="description" content="Ladder@Chinese Spam Email Classification Based on Naive Bayes Training and Testing Data This project primarily uses open-source data on GitHub. Data Processing First, we use regular expressions to filter the content of Chinese emails in the training set, removing all non-Chinese characters. The remaining content is then tokenized using jieba for word segmentation, and stopwords are filtered using a Chinese stopword list. The processed results for spam and normal emails">
    <meta name="author" content="NoneBack">
    <link rel="canonical" href="https://noneback.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%B8%AD%E6%96%87%E5%9E%83%E5%9C%BE%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB/">
        <meta name="google-site-verification" content="xxx">

    <link rel="alternate" type="application/rss+xml" href="https://noneback.github.io//index.xml" title="NoneBack">

    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-H0SRTJWPEK"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-H0SRTJWPEK', { 'anonymize_ip': false });
}
</script>



<script async defer data-website-id="43dc9e5a-7ab8-482e-94df-100975b5d2c8" src="https://umami-blog-pi.vercel.app/share/ZiNxRdFwotvSduLu/noneback.github.io"></script>

    <meta property="og:title" content="Chinese Spam Email Classification Based on Naive Bayes" />
<meta property="og:description" content="Chinese Spam Email Classification Based on Naive Bayes Training and Testing Data This project primarily uses open-source data on GitHub. Data Processing First, we use regular expressions to filter the content of Chinese emails in the training set, removing all non-Chinese characters. The remaining content is then tokenized using jieba for word segmentation, and stopwords are filtered using a Chinese stopword list. The processed results for spam and normal emails" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://noneback.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%B8%AD%E6%96%87%E5%9E%83%E5%9C%BE%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-06T00:00:00+00:00" />


<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Chinese Spam Email Classification Based on Naive Bayes"/>
<meta name="twitter:description" content="Chinese Spam Email Classification Based on Naive Bayes Training and Testing Data This project primarily uses open-source data on GitHub. Data Processing First, we use regular expressions to filter the content of Chinese emails in the training set, removing all non-Chinese characters. The remaining content is then tokenized using jieba for word segmentation, and stopwords are filtered using a Chinese stopword list. The processed results for spam and normal emails"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://noneback.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Chinese Spam Email Classification Based on Naive Bayes",
      "item": "https://noneback.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%B8%AD%E6%96%87%E5%9E%83%E5%9C%BE%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Chinese Spam Email Classification Based on Naive Bayes",
  "name": "Chinese Spam Email Classification Based on Naive Bayes",
  "description": "Chinese Spam Email Classification Based on Naive Bayes Training and Testing Data This project primarily uses open-source data on GitHub. Data Processing First, we use regular expressions to filter the content of Chinese emails in the training set, removing all non-Chinese characters. The remaining content is then tokenized using jieba for word segmentation, and stopwords are filtered using a Chinese stopword list. The processed results for spam and normal emails",
  "keywords": [
    "ML"
  ],
  "articleBody": "Chinese Spam Email Classification Based on Naive Bayes Training and Testing Data This project primarily uses open-source data on GitHub.\nData Processing First, we use regular expressions to filter the content of Chinese emails in the training set, removing all non-Chinese characters. The remaining content is then tokenized using jieba for word segmentation, and stopwords are filtered using a Chinese stopword list. The processed results for spam and normal emails are stored separately.\nTwo dictionaries, spam_voca and normal_voca, are used to store the word frequencies of different terms in different emails. The data processing is then complete.\nTraining and Prediction The training and prediction process involves calculating the probability $P(Spam|word_1, word_2, \\dots, word_n)$. When this probability exceeds a certain threshold, the email is classified as spam.\nBased on the conditional independence assumption of Naive Bayes, and assuming the prior probability $P(s) = P(s’) = 0.5$, we have:\n$P(s|w_1, w_2, \\dots, w_n) = \\frac{P(s, w_1, w_2, \\dots, w_n)}{P(w_1, w_2, \\dots, w_n)}$\n$= \\frac{P(w_1, w_2, \\dots, w_n | s) P(s)}{P(w_1, w_2, \\dots, w_n)} = \\frac{P(w_1, w_2, \\dots, w_n | s) P(s)}{P(w_1, w_2, \\dots, w_n | s) \\cdot p(s) + P(w_1, w_2, \\dots, w_n | s’) \\cdot p(s’)} $\nSince $P(spam) = P(not\\ spam)$, we have\n$\\frac{\\prod\\limits_{j=1}^n P(w_j | s)}{\\prod\\limits_{j=1}^n P(w_j | s) + \\prod\\limits_{j=1}^n P(w_j | s’)}$\nFurther, using Bayes’ theorem $P(w_j | s) = \\frac{P(s | w_j) \\cdot P(w_j)}{P(s)}$, the expression becomes\n$\\frac{\\prod\\limits_{j=1}^n P(s | w_j)}{\\prod\\limits_{j=1}^n P(s | w_j) + \\prod\\limits_{j=1}^n P(s’ | w_j)}$\nProcess details:\nFor each email in the test set, perform the same processing, and calculate the top $n$ words with the highest $P(s|w)$. During calculation, if a word appears only in the spam dictionary, set $P(w | s’) = 0.01$; similarly, if a word appears only in the normal dictionary, set $P(w | s) = 0.01$. If the word appears in neither, set $P(s|w) = 0.4$. These assumptions are based on prior research. Use the 15 most important words for each email and calculate the probability using the above formulas. If the probability is greater than the threshold $\\alpha$ (typically set to 0.9), classify it as spam; otherwise, classify it as a normal email. You can refer to the code for further details.\nResults By adjusting the number of words used for prediction, the best result for this dataset is:\nSelected 29 words: 0.9642857142857143 Project Structure data 中文停用词表.txt (Chinese stopword list) normal (folder for normal emails) spam (folder for spam emails) test (folder for test emails) main.py (main script) normal_voca.json (JSON file for normal email vocabulary) pycache (cache folder) utils.cpython-36.pyc spam_voca.json (JSON file for spam email vocabulary) utils.py (utility functions) Code # utils.py import jieba import numpy import re import os import json from collections import defaultdict spam_file_num = 7775 normal_file_num = 7063 # Load stopword list def get_stopwords(): return [i.strip() for i in open('./data/中文停用词表.txt', encoding='gbk')] # Read raw email content and process it def get_raw_str_list(path): stop_list = get_stopwords() with open(path, encoding='gbk') as f: raw_str = f.read() pattern = '[^\\u4E00-\\u9FA5]' # Chinese unicode range regex = re.compile(pattern) handled_str = re.sub(pattern, '', raw_str) str_list = [word for word in jieba.cut(handled_str) if word not in stop_list] return str_list # Build vocabulary def get_voca(path, is_file_path=False): if is_file_path: return read_voca_from_file(path) voca = defaultdict(int) file_list = [file for file in os.listdir(path)] for file in file_list: raw_str_list = get_raw_str_list(path + str(file)) for raw_str in raw_str_list: voca[raw_str] = voca[raw_str] + 1 return voca # Save vocabulary to JSON file def save_voca2json(voca, path, sort_by_value=False, indent_=4): if sort_by_value: sorted_by_value(voca) with open(path, 'w+') as f: f.write(json.dumps(voca, ensure_ascii=False, indent=indent_)) # Read vocabulary from JSON file def read_voca_from_file(path): with open(path) as f: voca = json.load(f) return voca # Sort dictionary by value def sorted_by_value(_dict): _dict = dict(sorted(spam_voca.items(), key=lambda x: x[1], reverse=True)) # Calculate P(Spam|word) def get_top_words_prob(path, spam_voca, normal_voca, words_size=30): critical_words = [] for word in get_raw_str_list(path): if word in spam_voca.keys() and word in normal_voca.keys(): p_w_s = spam_voca[word] / spam_file_num p_w_n = normal_voca[word] / normal_file_num p_s_w = p_w_s / (p_w_n + p_w_s) elif word in spam_voca.keys() and word not in normal_voca.keys(): p_w_s = spam_voca[word] / spam_file_num p_w_n = 0.01 p_s_w = p_w_s / (p_w_n + p_w_s) elif word not in spam_voca.keys() and word in normal_voca.keys(): p_w_s = 0.01 p_w_n = normal_voca[word] / normal_file_num p_s_w = p_w_s / (p_w_n + p_w_s) else: p_s_w = 0.4 critical_words.append([word, p_s_w]) return dict(sorted(critical_words[:words_size], key=lambda x: x[1], reverse=True)) # Calculate Bayesian probability def caculate_bayes(words_prob, spam_voca, normal_voca): p_s_w = 1 p_s_nw = 1 for word, prob in words_prob.items(): p_s_w *= prob p_s_nw *= (1 - prob) return p_s_w / (p_s_w + p_s_nw) def predict(bayes, threshold=0.9): return bayes \u003e= threshold # Get files and labels def get_files_labels(dir_path, is_spam=True): raw_files_list = os.listdir(dir_path) files_list = [dir_path + file for file in raw_files_list] labels = [is_spam for _ in range(len(files_list))] return files_list, labels # Predict and print results def predict_result(file_list, y, spam_voca, normal_voca, word_size=30): ret = [] right = 0 for file in file_list: words_prob = get_top_words_prob(file, spam_voca, normal_voca, words_size=word_size) bayes = caculate_bayes(words_prob, spam_voca, normal_voca) ret.append(predict(bayes)) for i in range(len(ret)): if ret[i] == y[i]: right += 1 print(right / len(y)) # main.py from utils import * if __name__ == '__main__': # Get vocabulary and save for future use spam_voca = get_voca('./spam_voca.json', is_file_path=True) normal_voca = get_voca('./normal_voca.json', is_file_path=True) save ",
  "wordCount" : "897",
  "inLanguage": "en",
  "datePublished": "2020-05-06T00:00:00Z",
  "dateModified": "2020-05-06T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "NoneBack"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://noneback.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%B8%AD%E6%96%87%E5%9E%83%E5%9C%BE%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "NoneBack",
    "logo": {
      "@type": "ImageObject",
      "url": "https://noneback.github.io/favicon.ico"
    }
  }
}
</script>
    <link rel="icon" href="/images/avatar.jpeg" sizes="16x16">

<link rel="apple-touch-icon" href="/images/avatar.jpeg">

<link rel="manifest" href="/images/avatar.jpeg">
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.7.0/style.css" />

    
    
    <link rel="stylesheet" href="/css/main.min.ec28f09e946fc0df77c187fcd0d0ebde58fca6de8efb8e1620f3d45c32d4da88.css" integrity="sha256-7CjwnpRvwN93wYf80NDr3lj8pt6O&#43;44WIPPUXDLU2og=" crossorigin="anonymous" media="screen" />

    
    <link rel="stylesheet" href="/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css" />

    
    <script src="/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js"></script>
    <script>hljs.highlightAll();</script>

    <script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script>
    </head>
<body>
      <main class="wrapper"><nav class="navigation">
    <section class="container">
        <a class="navigation-brand" href="/">
            HOME
        </a>
        <input type="checkbox" id="menu-toggle" />
        <label class="menu-button float-right" for="menu-toggle">
            <span></span><span></span><span></span>
        </label>
        
        <ul class="navigation-list" id="navigation-list">
            
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/posts">Blog</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/tags">Tags</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/archives">Archive</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="https://umami-blog-pi.vercel.app/share/ZiNxRdFwotvSduLu/noneback.github.io">Dashboard</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/about/">About</a>
            </li>
            
            

            <li class="navigation-item menu-separator">
                <span>|</span>
            </li>

            
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://github.com/noneback"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a>
            </li>
            
            

            <li class="navigation-item navigation-dark">
                <button id="mode" type="button" aria-label="toggle user light or dark theme">
                    <span class="toggle-dark"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></span>
                    <span class="toggle-light"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></span>
                </button>
            </li>

            
            
            
            
            
            
            
            <li class="navigation-item navigation-language">
                <a href="https://noneback.github.io/zh/">中</a>
            </li>
            
            
            
            
        </ul>
        
    </section>
</nav>
<div id="content">
<article class="blog-single">
  <header class="blog-title">
    <h1>Chinese Spam Email Classification Based on Naive Bayes</h1>
  </header>

  <p>
  <small>
    May 6, 2020&nbsp;· 897 words&nbsp;· 2 min</small>

  <small>
      
      ·
      
      
      <a href="https://noneback.github.io/tags/ml/">ML</a>
      
    </small>
  
<p>

  <div class="blog-toc">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#training-and-testing-data">Training and Testing Data</a></li>
    <li><a href="#data-processing">Data Processing</a></li>
    <li><a href="#training-and-prediction">Training and Prediction</a></li>
    <li><a href="#results">Results</a></li>
    <li><a href="#project-structure">Project Structure</a></li>
    <li><a href="#code">Code</a></li>
  </ul>
</nav>
  </div>

  <section class="blog-content"><h1 id="chinese-spam-email-classification-based-on-naive-bayes">Chinese Spam Email Classification Based on Naive Bayes</h1>
<h2 id="training-and-testing-data">Training and Testing Data</h2>
<p>This project primarily uses <a href="https://github.com/shijing888/BayesSpam">open-source data on GitHub</a>.</p>
<h2 id="data-processing">Data Processing</h2>
<p>First, we use regular expressions to filter the content of Chinese emails in the training set, removing all non-Chinese characters. The remaining content is then tokenized using <a href="https://github.com/fxsjy/jieba">jieba</a> for word segmentation, and stopwords are filtered using a Chinese stopword list. The processed results for spam and normal emails are stored separately.</p>
<p>Two dictionaries, <code>spam_voca</code> and <code>normal_voca</code>, are used to store the word frequencies of different terms in different emails. The data processing is then complete.</p>
<h2 id="training-and-prediction">Training and Prediction</h2>
<p>The training and prediction process involves calculating the probability $P(Spam|word_1, word_2, \dots, word_n)$. When this probability exceeds a certain threshold, the email is classified as spam.</p>
<blockquote>
<p>Based on the conditional independence assumption of Naive Bayes, and assuming the prior probability $P(s) = P(s&rsquo;) = 0.5$, we have:</p>
<p>$P(s|w_1, w_2, \dots, w_n) = \frac{P(s, w_1, w_2, \dots, w_n)}{P(w_1, w_2, \dots, w_n)}$</p>
<p>$= \frac{P(w_1, w_2, \dots, w_n | s) P(s)}{P(w_1, w_2, \dots, w_n)} = \frac{P(w_1, w_2, \dots, w_n | s) P(s)}{P(w_1, w_2, \dots, w_n | s) \cdot p(s) + P(w_1, w_2, \dots, w_n | s&rsquo;) \cdot p(s&rsquo;)} $</p>
<p>Since $P(spam) = P(not\ spam)$, we have</p>
<p>$\frac{\prod\limits_{j=1}^n P(w_j | s)}{\prod\limits_{j=1}^n P(w_j | s) + \prod\limits_{j=1}^n P(w_j | s&rsquo;)}$</p>
<p>Further, using Bayes&rsquo; theorem $P(w_j | s) = \frac{P(s | w_j) \cdot P(w_j)}{P(s)}$, the expression becomes</p>
<p>$\frac{\prod\limits_{j=1}^n P(s | w_j)}{\prod\limits_{j=1}^n P(s | w_j) + \prod\limits_{j=1}^n P(s&rsquo; | w_j)}$</p>
</blockquote>
<p>Process details:</p>
<ul>
<li>For each email in the test set, perform the same processing, and calculate the top $n$ words with the highest $P(s|w)$. During calculation, if a word appears only in the spam dictionary, set $P(w | s&rsquo;) = 0.01$; similarly, if a word appears only in the normal dictionary, set $P(w | s) = 0.01$. If the word appears in neither, set $P(s|w) = 0.4$. These assumptions are based on prior research.</li>
<li>Use the 15 most important words for each email and calculate the probability using the above formulas. If the probability is greater than the threshold $\alpha$ (typically set to 0.9), classify it as spam; otherwise, classify it as a normal email.</li>
</ul>
<p>You can refer to the code for further details.</p>
<h2 id="results">Results</h2>
<p>By adjusting the number of words used for prediction, the best result for this dataset is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Selected <span style="color:#ae81ff">29</span> words: <span style="color:#ae81ff">0.9642857142857143</span>
</span></span></code></pre></div><h2 id="project-structure">Project Structure</h2>
<ul>
<li><strong>data</strong>
<ul>
<li><code>中文停用词表.txt</code> (Chinese stopword list)</li>
<li><code>normal</code> (folder for normal emails)</li>
<li><code>spam</code> (folder for spam emails)</li>
<li><code>test</code> (folder for test emails)</li>
</ul>
</li>
<li><strong>main.py</strong> (main script)</li>
<li><strong>normal_voca.json</strong> (JSON file for normal email vocabulary)</li>
<li><strong><strong>pycache</strong></strong> (cache folder)
<ul>
<li><code>utils.cpython-36.pyc</code></li>
</ul>
</li>
<li><strong>spam_voca.json</strong> (JSON file for spam email vocabulary)</li>
<li><strong>utils.py</strong> (utility functions)</li>
</ul>
<h2 id="code">Code</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># utils.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> jieba
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spam_file_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">7775</span>
</span></span><span style="display:flex;"><span>normal_file_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">7063</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load stopword list</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_stopwords</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [i<span style="color:#f92672">.</span>strip() <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> open(<span style="color:#e6db74">&#39;./data/中文停用词表.txt&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gbk&#39;</span>)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Read raw email content and process it</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_raw_str_list</span>(path):
</span></span><span style="display:flex;"><span>    stop_list <span style="color:#f92672">=</span> get_stopwords()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(path, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gbk&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        raw_str <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>    pattern <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;[^</span><span style="color:#ae81ff">\u4E00</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\u9FA5</span><span style="color:#e6db74">]&#39;</span>  <span style="color:#75715e"># Chinese unicode range</span>
</span></span><span style="display:flex;"><span>    regex <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(pattern)
</span></span><span style="display:flex;"><span>    handled_str <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(pattern, <span style="color:#e6db74">&#39;&#39;</span>, raw_str)
</span></span><span style="display:flex;"><span>    str_list <span style="color:#f92672">=</span> [word <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> jieba<span style="color:#f92672">.</span>cut(handled_str) <span style="color:#66d9ef">if</span> word <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> stop_list]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> str_list
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build vocabulary</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_voca</span>(path, is_file_path<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> is_file_path:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> read_voca_from_file(path)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    voca <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    file_list <span style="color:#f92672">=</span> [file <span style="color:#66d9ef">for</span> file <span style="color:#f92672">in</span> os<span style="color:#f92672">.</span>listdir(path)]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> file <span style="color:#f92672">in</span> file_list:
</span></span><span style="display:flex;"><span>        raw_str_list <span style="color:#f92672">=</span> get_raw_str_list(path <span style="color:#f92672">+</span> str(file))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> raw_str <span style="color:#f92672">in</span> raw_str_list:
</span></span><span style="display:flex;"><span>            voca[raw_str] <span style="color:#f92672">=</span> voca[raw_str] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> voca
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save vocabulary to JSON file</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_voca2json</span>(voca, path, sort_by_value<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, indent_<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> sort_by_value:
</span></span><span style="display:flex;"><span>        sorted_by_value(voca)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(path, <span style="color:#e6db74">&#39;w+&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        f<span style="color:#f92672">.</span>write(json<span style="color:#f92672">.</span>dumps(voca, ensure_ascii<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, indent<span style="color:#f92672">=</span>indent_))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Read vocabulary from JSON file</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_voca_from_file</span>(path):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(path) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        voca <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>load(f)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> voca
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sort dictionary by value</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sorted_by_value</span>(_dict):
</span></span><span style="display:flex;"><span>    _dict <span style="color:#f92672">=</span> dict(sorted(spam_voca<span style="color:#f92672">.</span>items(), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>], reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate P(Spam|word)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_top_words_prob</span>(path, spam_voca, normal_voca, words_size<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>):
</span></span><span style="display:flex;"><span>    critical_words <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> get_raw_str_list(path):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> word <span style="color:#f92672">in</span> spam_voca<span style="color:#f92672">.</span>keys() <span style="color:#f92672">and</span> word <span style="color:#f92672">in</span> normal_voca<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>            p_w_s <span style="color:#f92672">=</span> spam_voca[word] <span style="color:#f92672">/</span> spam_file_num
</span></span><span style="display:flex;"><span>            p_w_n <span style="color:#f92672">=</span> normal_voca[word] <span style="color:#f92672">/</span> normal_file_num
</span></span><span style="display:flex;"><span>            p_s_w <span style="color:#f92672">=</span> p_w_s <span style="color:#f92672">/</span> (p_w_n <span style="color:#f92672">+</span> p_w_s)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> word <span style="color:#f92672">in</span> spam_voca<span style="color:#f92672">.</span>keys() <span style="color:#f92672">and</span> word <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> normal_voca<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>            p_w_s <span style="color:#f92672">=</span> spam_voca[word] <span style="color:#f92672">/</span> spam_file_num
</span></span><span style="display:flex;"><span>            p_w_n <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>            p_s_w <span style="color:#f92672">=</span> p_w_s <span style="color:#f92672">/</span> (p_w_n <span style="color:#f92672">+</span> p_w_s)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> word <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> spam_voca<span style="color:#f92672">.</span>keys() <span style="color:#f92672">and</span> word <span style="color:#f92672">in</span> normal_voca<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>            p_w_s <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>            p_w_n <span style="color:#f92672">=</span> normal_voca[word] <span style="color:#f92672">/</span> normal_file_num
</span></span><span style="display:flex;"><span>            p_s_w <span style="color:#f92672">=</span> p_w_s <span style="color:#f92672">/</span> (p_w_n <span style="color:#f92672">+</span> p_w_s)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            p_s_w <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>
</span></span><span style="display:flex;"><span>        critical_words<span style="color:#f92672">.</span>append([word, p_s_w])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dict(sorted(critical_words[:words_size], key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>], reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate Bayesian probability</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">caculate_bayes</span>(words_prob, spam_voca, normal_voca):
</span></span><span style="display:flex;"><span>    p_s_w <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    p_s_nw <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> word, prob <span style="color:#f92672">in</span> words_prob<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        p_s_w <span style="color:#f92672">*=</span> prob
</span></span><span style="display:flex;"><span>        p_s_nw <span style="color:#f92672">*=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> prob)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> p_s_w <span style="color:#f92672">/</span> (p_s_w <span style="color:#f92672">+</span> p_s_nw)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(bayes, threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> bayes <span style="color:#f92672">&gt;=</span> threshold
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get files and labels</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_files_labels</span>(dir_path, is_spam<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>    raw_files_list <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(dir_path)
</span></span><span style="display:flex;"><span>    files_list <span style="color:#f92672">=</span> [dir_path <span style="color:#f92672">+</span> file <span style="color:#66d9ef">for</span> file <span style="color:#f92672">in</span> raw_files_list]
</span></span><span style="display:flex;"><span>    labels <span style="color:#f92672">=</span> [is_spam <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(len(files_list))]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> files_list, labels
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and print results</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_result</span>(file_list, y, spam_voca, normal_voca, word_size<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>):
</span></span><span style="display:flex;"><span>    ret <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    right <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> file <span style="color:#f92672">in</span> file_list:
</span></span><span style="display:flex;"><span>        words_prob <span style="color:#f92672">=</span> get_top_words_prob(file, spam_voca, normal_voca, words_size<span style="color:#f92672">=</span>word_size)
</span></span><span style="display:flex;"><span>        bayes <span style="color:#f92672">=</span> caculate_bayes(words_prob, spam_voca, normal_voca)
</span></span><span style="display:flex;"><span>        ret<span style="color:#f92672">.</span>append(predict(bayes))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(ret)):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> ret[i] <span style="color:#f92672">==</span> y[i]:
</span></span><span style="display:flex;"><span>            right <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    print(right <span style="color:#f92672">/</span> len(y))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># main.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> utils <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get vocabulary and save for future use</span>
</span></span><span style="display:flex;"><span>    spam_voca <span style="color:#f92672">=</span> get_voca(<span style="color:#e6db74">&#39;./spam_voca.json&#39;</span>, is_file_path<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    normal_voca <span style="color:#f92672">=</span> get_voca(<span style="color:#e6db74">&#39;./normal_voca.json&#39;</span>, is_file_path<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    save
</span></span></code></pre></div></section>

  
  
  <div class="paginator">
    
    <a class="prev" href="https://noneback.github.io/posts/mit6.824-mapreduce/">
      <svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375C21.4339 3.93962 21.3581 3.30535 21.1917 2.76787M3.77086 21.1546C1.9934 20.7777 0.973585 18.7264 1.08749 16.688C1.2668 13.479 1.15721 9.43135 1.00513 6.21507C0.87809 3.52811 3.12891 1.16316 5.51029 1.25008C9.76594 1.40542 15.377 1.20229 18.7912 1.00542C20.0864 0.930734 20.8406 1.63385 21.1917 2.76787M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787C23.1097 4.18217 23.13 12.4191 22.9004 16.3608C20.8478 24.0194 12.3061 23.6662 6.5 22.0658M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608C21.2797 23.0494 11.3665 22.9511 6.5 22.0658M9.94496 9C9.28897 9.61644 7.63215 10.997 6.04814 11.7966C5.98257 11.8297 5.98456 11.9753 6.05061 12.0063C7.05496 12.4779 8.92941 13.9264 9.94496 15M6.44444 11.9667C8.86549 12.0608 14 12 16 11" stroke="currentColor" stroke-linecap="round"/>
      </svg>
      <span>MIT6.824-MapReduce</span></a>
    
    
    <a class="next" href="https://noneback.github.io/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%B9%B6%E8%A1%8C/"><span>Java Multithreading Programming</span>
      <svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375C21.4339 3.93962 21.3581 3.30535 21.1917 2.76787M3.77086 21.1546C1.9934 20.7777 0.973585 18.7264 1.08749 16.688C1.2668 13.479 1.15721 9.43135 1.00513 6.21507C0.87809 3.52811 3.12891 1.16316 5.51029 1.25008C9.76594 1.40542 15.377 1.20229 18.7912 1.00542C20.0864 0.930734 20.8406 1.63385 21.1917 2.76787M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787C23.1097 4.18217 23.13 12.4191 22.9004 16.3608C20.8478 24.0194 12.3061 23.6662 6.5 22.0658M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608C21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966C16.0174 11.8297 16.0154 11.9753 15.9494 12.0063C14.945 12.4779 13.0706 13.9264 12.055 15M15.5556 11.9667C13.1345 12.0608 8 12 6 11" stroke="currentColor" stroke-linecap="round"/>
      </svg>
    </a>
    
  </div>
  

  


</article>

        </div><footer class="footer">
  <p>&copy; 2024 <a href="https://noneback.github.io/">NoneBack</a>
    Powered by
    <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>
    <a href="https://github.com/guangzhengli/hugo-theme-ladder" rel="noopener" target="_blank">Ladder</a>
️  </p>
</footer>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211C22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257M21.7387 7.71865C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257C17.1684 -1.24629 7.83127 0.632493 4.27577 5.04257C2.88063 6.77451 -0.0433281 11.1668 1.38159 16.6571C2.27481 20.0988 5.17269 22.2936 8.19743 22.7725M20.7188 5.04257C22.0697 6.9404 24.0299 11.3848 22.3541 15.4153M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814C11.1703 6.98257 11.0247 6.98456 10.9937 7.05061C10.5221 8.05496 9.07362 9.92941 8 10.945M11.0333 7.44444C10.9392 9.86549 11 15 12 17" stroke="currentColor" stroke-linecap="round"/>
    </svg>
</a>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>

<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'Copy';

        function copyingDone() {
            copybutton.innerHTML = 'Copied';
            setTimeout(() => {
                copybutton.innerHTML = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });
        codeblock.parentNode.appendChild(copybutton);
    });
</script></main>
    </body><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <script>
      const images = Array.from(document.querySelectorAll(".blog-content img"));
      images.forEach(img => {
          mediumZoom(img, {
              margin: 10,  
              scrollOffset: 40,  
              container: null,  
              template: null,  
              background: 'rgba(0, 0, 0, 0.5)'
          });
      });
  </script>

  
  <script src="/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js" integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin="anonymous" defer></script></html>
